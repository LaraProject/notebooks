{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LaraProject_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-O6LyxnJ9pe",
        "colab_type": "text"
      },
      "source": [
        "**Entraînement du RNN de LaraProject via Google Colaboratory**\n",
        "\n",
        "Avant tout, assurez vous d'avoir plusieurs choses :\n",
        "\n",
        "*   Google Colaboratory doit être synchonisé avec Google Drive\n",
        "*   Le fichier data_facebook_cleaned.txt doit être à la racine de votre Google Drive\n",
        "*   Le fichier `word2vec_vectors.txt` (ou les fichiers `word2vec_model.bin*`) doit être à la racine de votre Google Drive\n",
        "*   Le Google Colab doit avoir accès à des GPUs (Exécution -> Modifer le type d'éxecution)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww3qWbr1JqFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers, activations, models, preprocessing\n",
        "from tensorflow.keras import preprocessing, utils\n",
        "import os\n",
        "import yaml\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "from gensim.models import KeyedVectors\n",
        "import argparse\n",
        "from gensim.models import FastText\n",
        "\n",
        "# Global variables\n",
        "\n",
        "# Make a TensorFlow Tokenizer\n",
        "tokenizer = preprocessing.text.Tokenizer(filters='')\n",
        "questions = []\n",
        "answers = []\n",
        "model_w2v = None\n",
        "embedding_matrix = None\n",
        "maxlen_questions = 0\n",
        "maxlen_answers = 0\n",
        "VOCAB_SIZE = 0\n",
        "vectors_size = 100\n",
        "\n",
        "# Import data\n",
        "def import_data():\n",
        "\t# Download data set\n",
        "\tr = requests.get('https://github.com/shubham0204/Dataset_Archives/blob/master/chatbot_nlp.zip?raw=true')\n",
        "\tz = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "\tz.extractall()\n",
        "\n",
        "# Import custom data\n",
        "def use_custom_data(path, size):\n",
        "\tglobal questions\n",
        "\tglobal answers\n",
        "\t# Open file\n",
        "\tf = open(path)\n",
        "\tlines = (f.readlines())\n",
        "\tlines = lines[:int(len(lines) * (size/100.))]\n",
        "\tf.close()\n",
        "\tnon_tonkenized_answers = []\n",
        "\tfor i in range(len(lines)):\n",
        "\t\tif i % 2 == 0:\n",
        "\t\t\tquestions.append(lines[i][11:-1])\n",
        "\t\telse:\n",
        "\t\t\tnon_tonkenized_answers.append(lines[i][9:-1])\n",
        "\n",
        "\t# Tokenize answers\n",
        "\tanswers = []\n",
        "\tfor i in range(len(non_tonkenized_answers)):\n",
        "\t\tanswers.append('<start> ' + non_tonkenized_answers[i] + ' <end>')\n",
        "\n",
        "\t# Force length\n",
        "\tlength_limit = 25\n",
        "\tnew_questions = []\n",
        "\tnew_answers = []\n",
        "\tfor i in range(len(questions)):\n",
        "\t\tif not((len(questions[i].split()) > length_limit) or (len(answers[i].split()) > length_limit+2)):\n",
        "\t\t\tnew_questions.append(questions[i])\n",
        "\t\t\tnew_answers.append(answers[i])\n",
        "\tquestions = new_questions\n",
        "\tanswers = new_answers\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_data():\n",
        "\tglobal questions\n",
        "\tglobal answers\n",
        "\n",
        "\tdir_path = 'chatbot_nlp/data'\n",
        "\tfiles_list = os.listdir(dir_path + os.sep)\n",
        "\n",
        "\t# Get questions and answers\n",
        "\tfor filepath in files_list:\n",
        "\t\tstream = open(dir_path + os.sep + filepath, 'rb')\n",
        "\t\tdocs = yaml.safe_load(stream)\n",
        "\t\tconversations = docs['conversations']\n",
        "\t\tfor con in conversations:\n",
        "\t\t\tif len(con) > 2:\n",
        "\t\t\t\tquestions.append(con[0])\n",
        "\t\t\t\treplies = con[1:]\n",
        "\t\t\t\tans = ''\n",
        "\t\t\t\tfor rep in replies:\n",
        "\t\t\t\t\tans += ' ' + rep\n",
        "\t\t\t\tanswers.append(ans)\n",
        "\t\t\telif len(con) > 1:\n",
        "\t\t\t\tquestions.append(con[0])\n",
        "\t\t\t\tanswers.append(con[1])\n",
        "\n",
        "\t# Filter out non-string questions\n",
        "\tanswers_with_tags = list()\n",
        "\tfor i in range(len(answers)):\n",
        "\t\tif type(answers[i]) == str:\n",
        "\t\t\tanswers_with_tags.append(answers[i])\n",
        "\t\telse:\n",
        "\t\t\tquestions.pop(i)\n",
        "\n",
        "\t# Tokenize answers\n",
        "\tanswers = list()\n",
        "\tfor i in range(len(answers_with_tags)):\n",
        "\t\tanswers.append('<start> ' + answers_with_tags[i] + ' <end>')\n",
        "\n",
        "# Prefilter before tokenizer\n",
        "def clean_text(text):\n",
        "\ttext = text.lower()\n",
        "\ttext = re.sub(r\"i'm\", 'i am', text)\n",
        "\ttext = re.sub(r\"he's\", 'he is', text)\n",
        "\ttext = re.sub(r\"it's\", 'it is', text)\n",
        "\ttext = re.sub(r\"she's\", 'she is', text)\n",
        "\ttext = re.sub(r\"that's\", 'that is', text)\n",
        "\ttext = re.sub(r\"what's\", 'what is', text)\n",
        "\ttext = re.sub(r\"where's\", 'where is', text)\n",
        "\ttext = re.sub(r\"how's\", 'how is', text)\n",
        "\ttext = re.sub(r\"\\'ll\", ' will', text)\n",
        "\ttext = re.sub(r\"\\'ve\", ' have', text)\n",
        "\ttext = re.sub(r\"\\'re\", ' are', text)\n",
        "\ttext = re.sub(r\"\\'d\", ' would', text)\n",
        "\ttext = re.sub(r\"n't\", ' not', text)\n",
        "\ttext = re.sub(r\"won't\", 'will not', text)\n",
        "\ttext = re.sub(r\"can't\", 'cannot', text)\n",
        "\treturn text\n",
        "\n",
        "def clean_everything():\n",
        "\tglobal questions\n",
        "\tglobal answers\n",
        "\tquestions = [clean_text(s) for s in questions]\n",
        "\tanswers = [clean_text(s) for s in answers]\n",
        "\n",
        "def load_word2vec(model_path, useFastText):\n",
        "\tglobal model_w2v\n",
        "\tif useFastText:\n",
        "\t\tmodel_w2v = FastText.load(model_path)\n",
        "\telse:\n",
        "\t\tmodel_w2v = KeyedVectors.load_word2vec_format(model_path, binary=False)\n",
        "\n",
        "def fit_tokenizer():\n",
        "\tglobal tokenizer\n",
        "\tglobal VOCAB_SIZE\n",
        "\ttokenizer.fit_on_texts(questions + answers)\n",
        "\tVOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "\t#print('VOCAB SIZE : {}'.format(VOCAB_SIZE))\n",
        "\n",
        "# Get all words which are in both the model and the dataset\n",
        "def get_known_words():\n",
        "\tknown_words = []\n",
        "\tfor word in tokenizer.word_index:\n",
        "\t\tif word in model_w2v:\n",
        "\t\t\tknown_words.append(word)\n",
        "\treturn known_words + [\"<unk>\"]\n",
        "\n",
        "# Make a new tokenizer\n",
        "def fit_new_tokenizer():\n",
        "\tglobal tokenizer\n",
        "\tglobal VOCAB_SIZE\n",
        "\tfit_tokenizer()\n",
        "\tknown_words = get_known_words()\n",
        "\ttokenizer_new = preprocessing.text.Tokenizer(oov_token='<unk>', filters='')\n",
        "\ttokenizer_new.fit_on_texts([known_words])\n",
        "\ttokenizer = tokenizer_new\n",
        "\tVOCAB_SIZE = len(tokenizer.word_index) + 1 + 1\n",
        "\t#print('VOCAB SIZE : {}'.format(VOCAB_SIZE))\n",
        "\n",
        "\n",
        "# Create the embedding matrix\n",
        "def create_embedding_matrix():\n",
        "\tglobal embedding_matrix\n",
        "\tembedding_matrix = np.zeros((VOCAB_SIZE, vectors_size))\n",
        "\tfor word, i in tokenizer.word_index.items():\n",
        "\t\tembedding_matrix[i] = model_w2v[word]\n",
        "\n",
        "# Create input and output datasets\n",
        "# encoder_input_data\n",
        "def create_input_output():\n",
        "\ttokenized_questions = tokenizer.texts_to_sequences(questions)\n",
        "\tmaxlen_questions = max([len(x) for x in tokenized_questions])\n",
        "\tpadded_questions = preprocessing.sequence.pad_sequences(tokenized_questions,\n",
        "\t\t\tmaxlen=maxlen_questions, padding='post')\n",
        "\tencoder_input_data = np.array(padded_questions)\n",
        "\t#print((encoder_input_data.shape, maxlen_questions))\n",
        "\n",
        "\t# decoder_input_data\n",
        "\ttokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "\tmaxlen_answers = max([len(x) for x in tokenized_answers])\n",
        "\tpadded_answers = preprocessing.sequence.pad_sequences(tokenized_answers,\n",
        "\t\t\tmaxlen=maxlen_answers, padding='post')\n",
        "\tdecoder_input_data = np.array(padded_answers)\n",
        "\t#print((decoder_input_data.shape, maxlen_answers))\n",
        "\n",
        "\t# decoder_output_data\n",
        "\ttokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "\tfor i in range(len(tokenized_answers)):\n",
        "\t\ttokenized_answers[i] = (tokenized_answers[i])[1:]\n",
        "\tpadded_answers = preprocessing.sequence.pad_sequences(tokenized_answers,\n",
        "\t\t\tmaxlen=maxlen_answers, padding='post')\n",
        "\tdecoder_output_data = np.array(padded_answers)\n",
        "\t#print(decoder_output_data.shape)\n",
        "\n",
        "\treturn encoder_input_data, decoder_input_data, decoder_output_data\n",
        "\n",
        "# Defining the Encoder-Decoder model\n",
        "def create_model(encoder_input_data, decoder_input_data, decoder_output_data, use_spatial_dropout=False, use_reccurent_dropout=False, use_batch_normalisation=False):\n",
        "\tencoder_inputs = tf.keras.layers.Input(shape=(None, ))\n",
        "\tencoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, vectors_size,\n",
        "\t\t\tweights=[embedding_matrix], trainable=False)(encoder_inputs)\n",
        "\tif use_batch_normalisation:\n",
        "\t\tencoder_embedding = tf.keras.layers.BatchNormalization()(encoder_embedding)\n",
        "\tif use_spatial_dropout:\n",
        "\t\tencoder_embedding = tf.keras.layers.SpatialDropout1D(0.2)(encoder_embedding)\n",
        "\t(encoder_outputs, state_h, state_c) = tf.keras.layers.LSTM(vectors_size,\n",
        "\t\t\treturn_state=True)(encoder_embedding)\n",
        "\tencoder_states = [state_h, state_c]\n",
        "\n",
        "\tdecoder_inputs = tf.keras.layers.Input(shape=(None, ))\n",
        "\tdecoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, vectors_size,\n",
        "\t\t\tweights=[embedding_matrix], trainable=False)(decoder_inputs)\n",
        "\tif use_batch_normalisation:\n",
        "\t\tdecoder_embedding = tf.keras.layers.BatchNormalization()(decoder_embedding)\n",
        "\tif use_spatial_dropout:\n",
        "\t\tdecoder_embedding = tf.keras.layers.SpatialDropout1D(0.2)(decoder_embedding)\n",
        "\tif use_reccurent_dropout:\n",
        "\t\tdecoder_lstm = tf.keras.layers.LSTM(vectors_size, return_state=True,\n",
        "\t\t\t\t\t\t\t\t\treturn_sequences=True, recurrent_dropout=0.2)\n",
        "\telse:\n",
        "\t\tdecoder_lstm = tf.keras.layers.LSTM(vectors_size, return_state=True,\n",
        "\t\t\t\t\t\t\t\t\t\t\treturn_sequences=True)\n",
        "\t(decoder_outputs, _, _) = decoder_lstm(decoder_embedding,\n",
        "\t\t\tinitial_state=encoder_states)\n",
        "\tdecoder_dense = tf.keras.layers.Dense(VOCAB_SIZE,\n",
        "\t\t\tactivation=tf.keras.activations.softmax)\n",
        "\toutput = decoder_dense(decoder_outputs)\n",
        "\n",
        "\tmodel = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)\n",
        "\tmodel.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "\t\t\t\t  loss='sparse_categorical_crossentropy')\n",
        "\tmodel.summary()\n",
        "\treturn model, encoder_inputs, encoder_states, decoder_embedding, decoder_lstm, decoder_dense, decoder_inputs\n",
        "\n",
        "# Training the model\n",
        "def train(use_spatial_dropout, use_reccurent_dropout, use_batch_normalisation):\n",
        "\tencoder_input_data, decoder_input_data, decoder_output_data = create_input_output()\n",
        "\tmodel, encoder_inputs, encoder_states, decoder_embedding, decoder_lstm, decoder_dense, decoder_inputs = create_model(encoder_input_data, decoder_input_data, decoder_output_data, use_spatial_dropout, use_reccurent_dropout, use_batch_normalisation)\n",
        "\tmodel.fit([encoder_input_data, decoder_input_data],\n",
        "\t\t\t  decoder_output_data, batch_size=512, epochs=500)\n",
        "\treturn encoder_inputs, encoder_states, decoder_embedding, decoder_lstm, decoder_dense, decoder_inputs\n",
        "\n",
        "# Defining inference models\n",
        "def make_inference_models(encoder_inputs, encoder_states, decoder_embedding, decoder_lstm, decoder_dense, decoder_inputs):\n",
        "\tencoder_model = tf.keras.models.Model(encoder_inputs,\n",
        "\t\t\tencoder_states)\n",
        "\tdecoder_state_input_h = tf.keras.layers.Input(shape=(vectors_size, ))\n",
        "\tdecoder_state_input_c = tf.keras.layers.Input(shape=(vectors_size, ))\n",
        "\tdecoder_states_inputs = [decoder_state_input_h,\n",
        "\t\t\t\t\t\t\t decoder_state_input_c]\n",
        "\t(decoder_outputs, state_h, state_c) = decoder_lstm(decoder_embedding,\n",
        "\t\t\t\t\t initial_state=decoder_states_inputs)\n",
        "\tdecoder_states = [state_h, state_c]\n",
        "\tdecoder_outputs = decoder_dense(decoder_outputs)\n",
        "\tdecoder_model = tf.keras.models.Model([decoder_inputs]\n",
        "\t\t\t+ decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\treturn (encoder_model, decoder_model)\n",
        "\n",
        "# Save the inference model\n",
        "def save_inference_model(path, encoder_inputs, encoder_states, decoder_embedding, decoder_lstm, decoder_dense, decoder_inputs):\n",
        "\t(encoder_model, decoder_model) = make_inference_models(encoder_inputs, encoder_states, decoder_embedding, decoder_lstm, decoder_dense, decoder_inputs)\n",
        "\tencoder_model.save(path + '/model_enc.h5')\n",
        "\tdecoder_model.save(path + '/model_dec.h5')\n",
        "\n",
        "# Save the tokenizer\n",
        "def save_tokenizer(path):\n",
        "\twith open(path + '/tokenizer.pickle', 'wb') as handle:\n",
        "\t\tpickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def save_length(path):\n",
        "\tdata = str(maxlen_questions) + \",\" + str(maxlen_answers)\n",
        "\twith open(path + \"/length.txt\", \"w\") as f:\n",
        "\t\tf.write(data)\n",
        "\n",
        "# Load the inference model\n",
        "def load_inference_model(enc_file, dec_file):\n",
        "\tencoder_model = tf.keras.models.load_model(enc_file)\n",
        "\tdecoder_model = tf.keras.models.load_model(dec_file)\n",
        "\treturn (encoder_model, decoder_model)\n",
        "\n",
        "# Load the tokenizer\n",
        "def load_tokenizer(tokenizer_file):\n",
        "\twith open(tokenizer_file, 'rb') as handle:\n",
        "\t\ttokenizer = pickle.load(handle)\n",
        "\treturn tokenizer\n",
        "\n",
        "def load_length(length_file):\n",
        "\twith open(length_file, \"r\") as f:\n",
        "\t\tdata = ((f.read()).split(\",\"))\n",
        "\treturn int(data[0]), int(data[1])\n",
        "\n",
        "# Talking with our Chatbot\n",
        "def str_to_tokens(sentence : str ):\n",
        "\twords = sentence.lower().split()\n",
        "\ttokens_list = list()\n",
        "\tfor word in words:\n",
        "\t\tif word in tokenizer.word_index:\n",
        "\t\t\ttokens_list.append(tokenizer.word_index[word])\n",
        "\t\telse:\n",
        "\t\t\ttokens_list.append(tokenizer.word_index['<unk>'])\n",
        "\treturn preprocessing.sequence.pad_sequences([tokens_list],\n",
        "\t\t\tmaxlen=maxlen_questions, padding='post')\n",
        "\n",
        "# Ask multiple questions\n",
        "def ask_questions(enc_model, dec_model):\n",
        "\tfor _ in range(10):\n",
        "\t\tstates_values = enc_model.predict(str_to_tokens(input('Enter question : ')))\n",
        "\t\tempty_target_seq = np.zeros((1, 1))\n",
        "\t\tempty_target_seq[0, 0] = tokenizer.word_index['<start>']\n",
        "\t\tstop_condition = False\n",
        "\t\tdecoded_translation = ''\n",
        "\t\twhile not stop_condition:\n",
        "\t\t\t(dec_outputs, h, c) = dec_model.predict([empty_target_seq]\n",
        "\t\t\t\t\t+ states_values)\n",
        "\t\t\tsampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "\t\t\tsampled_word = None\n",
        "\t\t\tfor (word, index) in tokenizer.word_index.items():\n",
        "\t\t\t\tif sampled_word_index == index:\n",
        "\t\t\t\t\tdecoded_translation += ' {}'.format(word)\n",
        "\t\t\t\t\tsampled_word = word\n",
        "\n",
        "\t\t\tif sampled_word == '<end>' or len(decoded_translation.split()) > maxlen_answers:\n",
        "\t\t\t\tstop_condition = True\n",
        "\n",
        "\t\t\tempty_target_seq = np.zeros((1, 1))\n",
        "\t\t\tempty_target_seq[0, 0] = sampled_word_index\n",
        "\t\t\tstates_values = [h, c]\n",
        "\n",
        "\t\tprint(decoded_translation[:-5])  # remove end w\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtJ4CwC85PWX",
        "colab_type": "text"
      },
      "source": [
        "**Traitement des données et entraînement**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2HJLtFdK85p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importation des données\n",
        "use_custom_data(\"/content/drive/My Drive/data_facebook_cleaned.txt\", 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERSyPqi149Np",
        "colab_type": "text"
      },
      "source": [
        "Si vous rencontrez des problèmes de taille de \"tensor\" dans la suite, il est nécessaire de diminuer le pourcentage de données utilisé (par defaut `100`) dans la commande précédente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2n1B3Pu5br1",
        "colab_type": "text"
      },
      "source": [
        "**Note pour le chargement du modèle Word2Vec**\n",
        "\n",
        "Il y a plusieurs approches disponibles pour le word2vec :\n",
        "\n",
        "\n",
        "*   Si vous avez le fichier `word2vec_vectors.tx` à la racine de votre Google Drive, alors vous pouvez utiliser l'approche 1\n",
        "*   Si vous avez les 7 fichiers de la forme `word2vec_model.bin*` à la racine de votre Google Drive, alors vous pouvez utiliser l'approche 2\n",
        "\n",
        "Il faut choisir entre l'approche 1 et l'approche 2. L'approche 2 est significativement plus performante mais plus lente (~5h30 d'entraînement).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpu-UQ1x4yMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chargement du modèle Word2Vec (approche 1)\n",
        "load_word2vec(\"/content/drive/My Drive/word2vec_vectors.txt\", False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JtZWeyT5sPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chargement du modèle Word2Vec (approche 2)\n",
        "load_word2vec(\"/content/drive/My Drive/word2vec_model.bin\", True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa_TMo9z4rmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Création du tokenizer et de la matrice d'intégration\n",
        "fit_new_tokenizer()\n",
        "create_embedding_matrix()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdN_OoWaLOAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lancement de l'entraînement\n",
        "encoder_inputs, encoder_states, decoder_embedding, decoder_lstm, decoder_dense, decoder_inputs = train(False, False, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCbqe0MmLT82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sauvegarde\n",
        "save_inference_model(\"/content/\", encoder_inputs, encoder_states, decoder_embedding, decoder_lstm, decoder_dense, decoder_inputs)\n",
        "save_tokenizer(\"/content/\")\n",
        "save_length(\"/content/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4vbv0lOLXG1",
        "colab_type": "text"
      },
      "source": [
        "**Téléchargez les 4 fichiers (`length.txt`, `model_dec.h5`, `model_enc.h5` et  `tokenizer.pickle`) depuis le dossier `/content/`**\n"
      ]
    }
  ]
}